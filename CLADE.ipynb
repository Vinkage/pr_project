{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CLADE.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "qycfHLHqkm3G"
      ],
      "toc_visible": true,
      "mount_file_id": "15ALT7dAG71fnBVMpe63bz6zk_j5uWcHT",
      "authorship_tag": "ABX9TyM2ZpF+m8oUWxa8QkYvhiIq"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3KHj1kX7z_B2",
        "outputId": "5b950ffc-4322-4644-a38d-1b354aa8df9c"
      },
      "source": [
        "# Check nvidia and nvcc cuda compiler\r\n",
        "\r\n",
        "!nvidia-smi\r\n",
        "!/usr/local/cuda/bin/nvcc --version"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Wed Jan 13 18:30:21 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.27.04    Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   40C    P8     9W /  70W |      0MiB / 15079MiB |      0%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n",
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2019 NVIDIA Corporation\n",
            "Built on Sun_Jul_28_19:07:16_PDT_2019\n",
            "Cuda compilation tools, release 10.1, V10.1.243\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qycfHLHqkm3G"
      },
      "source": [
        "# other"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WjZ2jJyr573k",
        "outputId": "5f57eb3b-3440-4d96-c5c3-99f91c3cc381"
      },
      "source": [
        "#1 - setup ssh/user\r\n",
        "\r\n",
        "\r\n",
        "#Generate a random root password\r\n",
        "import random, string\r\n",
        "password = ''.join(random.choice(string.ascii_letters + string.digits) for i in range(30))\r\n",
        "\r\n",
        "\r\n",
        "#Setup sshd\r\n",
        "! apt-get install -qq -o=Dpkg::Use-Pty=0 openssh-server pwgen > /dev/null\r\n",
        "\r\n",
        "#Set root password\r\n",
        "! echo root:$password | chpasswd\r\n",
        "! mkdir -p /var/run/sshd\r\n",
        "! echo \"PermitRootLogin yes\" >> /etc/ssh/sshd_config\r\n",
        "! echo \"PasswordAuthentication yes\" >> /etc/ssh/sshd_config\r\n",
        "\r\n",
        "print(\"username: root\")\r\n",
        "print(\"password: \", password)\r\n",
        "\r\n",
        "#Run sshd\r\n",
        "get_ipython().system_raw('/usr/sbin/sshd -D &')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "username: root\n",
            "password:  ZdpHv30jxlCj165DIlnFtJl0l2HunC\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TQuLE8oc5_8J"
      },
      "source": [
        "# 2 - Download Ngrok\r\n",
        "\r\n",
        "! wget -q -c -nc https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\r\n",
        "! unzip -qq -n ngrok-stable-linux-amd64.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yluMMWSN6B9G",
        "outputId": "4e3a5e11-ecd8-4686-9952-877dc4a0d5e0"
      },
      "source": [
        "# 3 - setup Ngrok - authtoken\r\n",
        "\r\n",
        "#Ask token\r\n",
        "print(\"Get your authtoken from https://dashboard.ngrok.com/auth\")\r\n",
        "import getpass\r\n",
        "authtoken = getpass.getpass()\r\n",
        "\r\n",
        "#Create tunnel\r\n",
        "get_ipython().system_raw('./ngrok authtoken $authtoken && ./ngrok tcp 22 &')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Get your authtoken from https://dashboard.ngrok.com/auth\n",
            "··········\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MTWCZ4sZ6GNs",
        "outputId": "8b924a30-f2ea-4e5d-d188-225c74cd1db9"
      },
      "source": [
        "# When done with the ssh, kill Ngrok\r\n",
        "\r\n",
        "!kill $(ps aux | grep './ngrok' | awk '{print $2}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "^C\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gUCACM6rktEj"
      },
      "source": [
        "# Clade"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zcd6HrKICuN-"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mdutxc2H57Wv"
      },
      "source": [
        "# !git clone https://github.com/tzt101/CLADE.git CLADE\r\n",
        "# !rm -rf pr_proj\r\n",
        "!git clone -b main https://github.com/Vinkage/pr_project.git pr_proj\r\n",
        "# file = \"/content/CLADE/requirements.txt\"\r\n",
        "# with open(file) as f:\r\n",
        "#   lines = f.readlines()\r\n",
        "# lines[0] = \"torch==1.6.0\\n\"\r\n",
        "# with open(file, \"w\") as f:\r\n",
        "#   f.writelines(lines)\r\n",
        "\r\n",
        "!pip install -r pr_proj/SPADE/requirements.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WDJ4L2vZSZbX",
        "outputId": "ab8dba94-1dc5-4e79-9881-b0696c94c149"
      },
      "source": [
        "# !pip install torch --upgrade\r\n",
        "# !pip install torchvision\r\n",
        "# dominate>=2.3.1\r\n",
        "# !pip install dill\r\n",
        "# !pip install scikit-image\r\n",
        "# !pip install dominate"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting dominate\n",
            "  Downloading https://files.pythonhosted.org/packages/ef/a8/4354f8122c39e35516a2708746d89db5e339c867abbd8e0179bccee4b7f9/dominate-2.6.0-py2.py3-none-any.whl\n",
            "Installing collected packages: dominate\n",
            "Successfully installed dominate-2.6.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-qn1lN8MtSq0",
        "outputId": "84f72ca9-8795-458f-903e-e06ec77e18df"
      },
      "source": [
        "# !python /content/pr_proj/SPADE/eval.py \r\n",
        "!python /content/pr_proj/SPADE/eval.py --name mock_gridsearch --dataset \"/content/pr_proj/mock_data/\" --root_path \"/content/pr_proj/SPADE/\"\r\n",
        "\r\n",
        "# !python /content/pr_proj/CLADE/calc.py --name Rick_test_adek_subset_small3 --dataset_mode ade20k --dataroot \"Datasets/ADEChallengeData2016/\" --checkpoints_dir \"drive/MyDrive/Pattern recognition project/Checkpoints/\"\r\n",
        "# !python pr_proj/CLADE/train.py --name Rick_test_adek_subset_small2 --dataset_mode ade20k --dataroot \"Datasets/ADEChallengeData2016/\" --checkpoints_dir \"drive/MyDrive/Pattern recognition project/Checkpoints/\""
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'name': 'mock_gridsearch', 'dataset_path': '/content/pr_proj/mock_data/', 'root_path': '/content/pr_proj/SPADE/'}\n",
            "created temporary directory, resizing images and saving training FID stats /tmp/tmpd1c7d62k\n",
            "Warning: batch size is bigger than the data size. Setting batch size to data size\n",
            "100% 1/1 [00:00<00:00,  3.89it/s]\n",
            "dataset [CustomDataset] of size 10 was created\n",
            "Network [SPADEGenerator] was created. Total number of parameters: 92.5 million. To see the architecture, do print(network).\n",
            "Network [MultiscaleDiscriminator] was created. Total number of parameters: 1.4 million. To see the architecture, do print(network).\n",
            "create web directory ./checkpoints/mock_gridsearch_search1/web...\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "End of epoch 1 / 1 \t Time Taken: 6 sec\n",
            "Saved current iteration count at ./checkpoints/mock_gridsearch_search1/iter.txt.\n",
            "saving the model at the end of epoch 1, iters 10\n",
            "Training was successfully finished.\n",
            "dataset [CustomDataset] of size 10 was created\n",
            "Network [SPADEGenerator] was created. Total number of parameters: 92.5 million. To see the architecture, do print(network).\n",
            "process image... /content/pr_proj/mock_data/test/images/488.jpg\n",
            "process image... /content/pr_proj/mock_data/test/images/1684.jpg\n",
            "process image... /content/pr_proj/mock_data/test/images/2097.jpg\n",
            "process image... /content/pr_proj/mock_data/test/images/2269.jpg\n",
            "process image... /content/pr_proj/mock_data/test/images/5659.jpg\n",
            "process image... /content/pr_proj/mock_data/test/images/6257.jpg\n",
            "process image... /content/pr_proj/mock_data/test/images/9532.jpg\n",
            "process image... /content/pr_proj/mock_data/test/images/9614.jpg\n",
            "process image... /content/pr_proj/mock_data/test/images/13230.jpg\n",
            "process image... /content/pr_proj/mock_data/test/images/22095.jpg\n",
            "Warning: batch size is bigger than the data size. Setting batch size to data size\n",
            "100% 1/1 [00:00<00:00,  4.40it/s]\n",
            "dataset [CustomDataset] of size 10 was created\n",
            "Network [SPADEGenerator] was created. Total number of parameters: 92.5 million. To see the architecture, do print(network).\n",
            "Network [MultiscaleDiscriminator] was created. Total number of parameters: 1.4 million. To see the architecture, do print(network).\n",
            "create web directory ./checkpoints/mock_gridsearch_search2/web...\n",
            "End of epoch 1 / 1 \t Time Taken: 6 sec\n",
            "Saved current iteration count at ./checkpoints/mock_gridsearch_search2/iter.txt.\n",
            "saving the model at the end of epoch 1, iters 10\n",
            "Training was successfully finished.\n",
            "dataset [CustomDataset] of size 10 was created\n",
            "Network [SPADEGenerator] was created. Total number of parameters: 92.5 million. To see the architecture, do print(network).\n",
            "process image... /content/pr_proj/mock_data/test/images/488.jpg\n",
            "process image... /content/pr_proj/mock_data/test/images/1684.jpg\n",
            "process image... /content/pr_proj/mock_data/test/images/2097.jpg\n",
            "process image... /content/pr_proj/mock_data/test/images/2269.jpg\n",
            "process image... /content/pr_proj/mock_data/test/images/5659.jpg\n",
            "process image... /content/pr_proj/mock_data/test/images/6257.jpg\n",
            "process image... /content/pr_proj/mock_data/test/images/9532.jpg\n",
            "process image... /content/pr_proj/mock_data/test/images/9614.jpg\n",
            "process image... /content/pr_proj/mock_data/test/images/13230.jpg\n",
            "process image... /content/pr_proj/mock_data/test/images/22095.jpg\n",
            "Warning: batch size is bigger than the data size. Setting batch size to data size\n",
            "100% 1/1 [00:00<00:00,  4.13it/s]\n",
            "dataset [CustomDataset] of size 10 was created\n",
            "Network [SPADEGenerator] was created. Total number of parameters: 92.5 million. To see the architecture, do print(network).\n",
            "Network [MultiscaleDiscriminator] was created. Total number of parameters: 1.4 million. To see the architecture, do print(network).\n",
            "create web directory ./checkpoints/mock_gridsearch_search3/web...\n",
            "End of epoch 1 / 1 \t Time Taken: 6 sec\n",
            "Saved current iteration count at ./checkpoints/mock_gridsearch_search3/iter.txt.\n",
            "saving the model at the end of epoch 1, iters 10\n",
            "Training was successfully finished.\n",
            "dataset [CustomDataset] of size 10 was created\n",
            "Network [SPADEGenerator] was created. Total number of parameters: 92.5 million. To see the architecture, do print(network).\n",
            "process image... /content/pr_proj/mock_data/test/images/488.jpg\n",
            "process image... /content/pr_proj/mock_data/test/images/1684.jpg\n",
            "process image... /content/pr_proj/mock_data/test/images/2097.jpg\n",
            "process image... /content/pr_proj/mock_data/test/images/2269.jpg\n",
            "process image... /content/pr_proj/mock_data/test/images/5659.jpg\n",
            "process image... /content/pr_proj/mock_data/test/images/6257.jpg\n",
            "process image... /content/pr_proj/mock_data/test/images/9532.jpg\n",
            "process image... /content/pr_proj/mock_data/test/images/9614.jpg\n",
            "process image... /content/pr_proj/mock_data/test/images/13230.jpg\n",
            "process image... /content/pr_proj/mock_data/test/images/22095.jpg\n",
            "Warning: batch size is bigger than the data size. Setting batch size to data size\n",
            "100% 1/1 [00:00<00:00,  4.28it/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8A_j4zoYgLFx"
      },
      "source": [
        "# Datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GDZlzcAjgQNb"
      },
      "source": [
        "# from google.colab import drive\r\n",
        "# drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "drhBeGCrNR-M"
      },
      "source": [
        "# !python CLADE/train.py --name Rick_test_adek_subset_small2 --dataset_mode ade20k --dataroot \"Datasets/ADEChallengeData2016/\" --checkpoints_dir \"drive/MyDrive/Pattern recognition project/Checkpoints/\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_yczSMbECUz5"
      },
      "source": [
        "# !rm -rf sample_data/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oIBJfzIaG-76"
      },
      "source": [
        "import urllib.request\r\n",
        "!mkdir Datasets\r\n",
        "# urllib.request.urlretrieve(\"http://data.csail.mit.edu/places/ADEchallenge/ADEChallengeData2016.zip\", \"Datasets/ADEChallengeData2016.zip\")\r\n",
        "# urllib.request.urlretrieve(\"http://images.cocodataset.org/zips/train2017.zip\", \"Datasets/train2017.zip\")\r\n",
        "# urllib.request.urlretrieve(\"http://images.cocodataset.org/annotations/annotations_trainval2017.zip\", \"Datasets/annotations_trainval2017.zip\")\r\n",
        "# urllib.request.urlretrieve(\"http://images.cocodataset.org/zips/val2017.zip\", \"Datasets/val2017.zip\")\r\n",
        "# urllib.request.urlretrieve(\"https://mab.to/7cnM68u20\", \"Datasets/city.zip\")\r\n",
        "\r\n",
        "\r\n",
        "# From drive\r\n",
        "# urllib.request.urlretrieve(\"http://calvin.inf.ed.ac.uk/wp-content/uploads/data/cocostuffdataset/stuffthingmaps_trainval2017.zip\", \"Datasets/stuffthingmaps_trainval2017.zip\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CqnwCM0F6BTf"
      },
      "source": [
        "import zipfile\r\n",
        "# with zipfile.ZipFile(\"/content/Datasets/ADEChallengeData2016.zip\", 'r') as zip_ref:\r\n",
        "#   zip_ref.extractall(\"/content/Datasets/\")\r\n",
        "\r\n",
        "# with zipfile.ZipFile(\"/content/Datasets/train2017.zip\", 'r') as zip_ref:\r\n",
        "#   zip_ref.extractall(\"/content/Datasets/\")\r\n",
        "\r\n",
        "# with zipfile.ZipFile(\"/content/Datasets/annotations_trainval2017.zip\", 'r') as zip_ref:\r\n",
        "#   zip_ref.extractall(\"/content/Datasets/\")\r\n",
        "\r\n",
        "# with zipfile.ZipFile(\"/content/Datasets/ADEChallengeData2016.zip\", 'r') as zip_ref:\r\n",
        "#   zip_ref.extractall(\"/content/Datasets/\")\r\n",
        "\r\n",
        "with zipfile.ZipFile(\"/content/drive/MyDrive/Pattern_recognition_project/Datasets/ADEChallengeData2016.zip\", 'r') as zip_ref:\r\n",
        "  zip_ref.extractall(\"/content/Datasets/\")\r\n",
        "\r\n",
        "# with zipfile.ZipFile(\"drive/MyDrive/Pattern recognition project/Datasets/gtFine_trainvaltest.zip\", 'r') as zip_ref:\r\n",
        "#   zip_ref.extractall(\"/content/Datasets/\")\r\n",
        "\r\n",
        "# with zipfile.ZipFile(\"drive/MyDrive/Pattern recognition project/Datasets/stuffthingmaps_trainval2017.zip\", 'r') as zip_ref:\r\n",
        "#   zip_ref.extractall(\"/content/Datasets/stuff/\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QA-SsgYsvd0Z"
      },
      "source": [
        "file = \"drive/MyDrive/Pattern recognition project/Datasets/subsets/ADEChallengeData2016/sceneCategories.txt\"\r\n",
        "import os\r\n",
        "translate = {}\r\n",
        "translate['train'] = 'training'\r\n",
        "translate['val'] = 'validation'\r\n",
        "with open(file) as f:\r\n",
        "  lines = f.readlines()\r\n",
        "\r\n",
        "categories = {}\r\n",
        "for line in lines:\r\n",
        "  file_name, category = line.split()\r\n",
        "  splitten = file_name.split(\"_\")\r\n",
        "  subcat = splitten[1]\r\n",
        "\r\n",
        "  if subcat not in categories:\r\n",
        "    categories[subcat] = {}\r\n",
        "\r\n",
        "  if category not in categories[subcat]:\r\n",
        "    categories[subcat][category] = []\r\n",
        "\r\n",
        "  categories[subcat][category].append(file_name)\r\n",
        "  # categories[category][file_name] = 0\r\n",
        "\r\n",
        "print(lines)\r\n",
        "\r\n",
        "n = 2\r\n",
        "for i, category in categories.items():\r\n",
        "  for key, values in category.items():\r\n",
        "    count = 0\r\n",
        "    print(key, \": \", len(values))\r\n",
        "    for value in values:\r\n",
        "      if count < n:\r\n",
        "        count += 1\r\n",
        "        continue\r\n",
        "\r\n",
        "      # test = lines.index(value + \" \" + key + \"\\n\")\r\n",
        "      # del lines[test]\r\n",
        "\r\n",
        "      # print(\"drive/MyDrive/Pattern recognition project/Datasets/subsets/ADEChallengeData2016/annotations/\" + translate[i] + \"/\" + value + \".png\")\r\n",
        "      # print(\"drive/MyDrive/Pattern recognition project/Datasets/subsets/ADEChallengeData2016/images/\" + translate[i] + \"/\" + value + \".jpg\")\r\n",
        "      # print(translate[i])\r\n",
        "\r\n",
        "      # os.remove(\"drive/MyDrive/Pattern recognition project/Datasets/subsets/ADEChallengeData2016/annotations/\" + translate[i] + \"/\" + value + \".png\")\r\n",
        "      # os.remove(\"drive/MyDrive/Pattern recognition project/Datasets/subsets/ADEChallengeData2016/images/\" + translate[i] + \"/\" + value + \".jpg\")\r\n",
        "\r\n",
        "# print(lines)  \r\n",
        "\r\n",
        "with open(file, 'w') as file:\r\n",
        "  file.writelines(lines)\r\n",
        "\r\n",
        "\r\n",
        "# print(categories)\r\n",
        "# lines[0] = \"torch==1.6.0\\n\"\r\n",
        "# with open(file, \"w\") as f:\r\n",
        "#   f.writelines(lines)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QkXVznzrF8Xs"
      },
      "source": [
        "\r\n",
        "# shutil.copytree('drive/MyDrive/Pattern recognition project/Datasets/subsets/ADEChallengeData2016', '/content/Datasets/ADEChallengeData2016')\r\n",
        "# shutil.copytree('baz', 'foo', dirs_exist_ok=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D2gapJZwgT2O",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 420
        },
        "outputId": "7b213f99-986b-4f3c-bfb0-1b6748c14f7b"
      },
      "source": [
        "# !python CLADE/datasets/coco_generate_instance_map.py --annotation_file=\"Datasets/annotations/instances_train2017.json\" --input_label_dir=\"Datasets/stuff/train2017/\" --output_instance_dir=\"Datasets/Output/\"\r\n",
        "\r\n",
        "import shutil\r\n",
        "# shutil.make_archive(\"drive/MyDrive/Pattern recognition project/Datasets/subsets/subset\", 'zip', \"drive/MyDrive/Pattern recognition project/Datasets/subsets/ADEChallengeData2016/\")\r\n",
        "# !zip -r /content/file.zip /content/Folder_To_Zip\r\n",
        "\r\n",
        "# from google.colab import files\r\n",
        "# files.download(\"/content/output.zip\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-98352f88f783>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mshutil\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mshutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_archive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"drive/MyDrive/Pattern recognition project/Datasets/subsets/subset\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'zip'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"drive/MyDrive/Pattern recognition project/Datasets/subsets/ADEChallengeData2016/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;31m# !zip -r /content/file.zip /content/Folder_To_Zip\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/shutil.py\u001b[0m in \u001b[0;36mmake_archive\u001b[0;34m(base_name, format, root_dir, base_dir, verbose, dry_run, owner, group, logger)\u001b[0m\n\u001b[1;32m    804\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    805\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 806\u001b[0;31m         \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbase_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    807\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    808\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mroot_dir\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/shutil.py\u001b[0m in \u001b[0;36m_make_zipfile\u001b[0;34m(base_name, base_dir, verbose, dry_run, logger)\u001b[0m\n\u001b[1;32m    702\u001b[0m                     \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormpath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 704\u001b[0;31m                         \u001b[0mzf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    705\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0mlogger\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    706\u001b[0m                             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"adding '%s'\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/zipfile.py\u001b[0m in \u001b[0;36mwrite\u001b[0;34m(self, filename, arcname, compress_type)\u001b[0m\n\u001b[1;32m   1644\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1645\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzinfo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mdest\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1646\u001b[0;31m                 \u001b[0mshutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopyfileobj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1024\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1647\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1648\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwritestr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzinfo_or_arcname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompress_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/shutil.py\u001b[0m in \u001b[0;36mcopyfileobj\u001b[0;34m(fsrc, fdst, length)\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[0;34m\"\"\"copy data from file-like object fsrc to file-like object fdst\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m         \u001b[0mbuf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfsrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlength\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mbuf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hM1Qq0hqAiZ7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3c93b9bf-d536-41b9-abc1-b08e32a971d1"
      },
      "source": [
        "!pip install python-rclone"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting python-rclone\n",
            "  Downloading https://files.pythonhosted.org/packages/0e/fd/55fc62562c8c6eff7f9a9a9c2a35167ad736362d09ec49c8f4d47586e808/python_rclone-0.0.2-py3-none-any.whl\n",
            "Installing collected packages: python-rclone\n",
            "Successfully installed python-rclone-0.0.2\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}